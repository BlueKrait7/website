{
  "slug": "conjugate-gradient",
  "name": "Conjugate Gradient",
  "categories": [
    "linearalgebra"
  ],
  "body": {},
  "implementations": {
    "python": {
      "dir": "linear_algebra\\src\\conjugate_gradient.py",
      "url": "https://github.com/TheAlgorithms/python/tree/master/linear_algebra\\src\\conjugate_gradient.py",
      "code": "<span class=\"hljs-string\">&quot;&quot;&quot;\r\nResources:\r\n- https://en.wikipedia.org/wiki/Conjugate_gradient_method\r\n- https://en.wikipedia.org/wiki/Definite_symmetric_matrix\r\n&quot;&quot;&quot;</span>\r\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Any</span>\r\n\r\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\r\n\r\n\r\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_is_matrix_spd</span>(<span class=\"hljs-params\">matrix: np.ndarray</span>) -&gt; <span class=\"hljs-built_in\">bool</span>:\r\n    <span class=\"hljs-string\">&quot;&quot;&quot;\r\n    Returns True if input matrix is symmetric positive definite.\r\n    Returns False otherwise.\r\n\r\n    For a matrix to be SPD, all eigenvalues must be positive.\r\n\r\n    &gt;&gt;&gt; import numpy as np\r\n    &gt;&gt;&gt; matrix = np.array([\r\n    ... [4.12401784, -5.01453636, -0.63865857],\r\n    ... [-5.01453636, 12.33347422, -3.40493586],\r\n    ... [-0.63865857, -3.40493586,  5.78591885]])\r\n    &gt;&gt;&gt; _is_matrix_spd(matrix)\r\n    True\r\n    &gt;&gt;&gt; matrix = np.array([\r\n    ... [0.34634879,  1.96165514,  2.18277744],\r\n    ... [0.74074469, -1.19648894, -1.34223498],\r\n    ... [-0.7687067 ,  0.06018373, -1.16315631]])\r\n    &gt;&gt;&gt; _is_matrix_spd(matrix)\r\n    False\r\n    &quot;&quot;&quot;</span>\r\n    <span class=\"hljs-comment\"># Ensure matrix is square.</span>\r\n    <span class=\"hljs-keyword\">assert</span> np.shape(matrix)[<span class=\"hljs-number\">0</span>] == np.shape(matrix)[<span class=\"hljs-number\">1</span>]\r\n\r\n    <span class=\"hljs-comment\"># If matrix not symmetric, exit right away.</span>\r\n    <span class=\"hljs-keyword\">if</span> np.allclose(matrix, matrix.T) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">False</span>:\r\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">False</span>\r\n\r\n    <span class=\"hljs-comment\"># Get eigenvalues and eignevectors for a symmetric matrix.</span>\r\n    eigen_values, _ = np.linalg.eigh(matrix)\r\n\r\n    <span class=\"hljs-comment\"># Check sign of all eigenvalues.</span>\r\n    <span class=\"hljs-comment\"># np.all returns a value of type np.bool_</span>\r\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">bool</span>(np.<span class=\"hljs-built_in\">all</span>(eigen_values &gt; <span class=\"hljs-number\">0</span>))\r\n\r\n\r\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_create_spd_matrix</span>(<span class=\"hljs-params\">dimension: <span class=\"hljs-built_in\">int</span></span>) -&gt; <span class=\"hljs-type\">Any</span>:\r\n    <span class=\"hljs-string\">&quot;&quot;&quot;\r\n    Returns a symmetric positive definite matrix given a dimension.\r\n\r\n    Input:\r\n    dimension gives the square matrix dimension.\r\n\r\n    Output:\r\n    spd_matrix is an diminesion x dimensions symmetric positive definite (SPD) matrix.\r\n\r\n    &gt;&gt;&gt; import numpy as np\r\n    &gt;&gt;&gt; dimension = 3\r\n    &gt;&gt;&gt; spd_matrix = _create_spd_matrix(dimension)\r\n    &gt;&gt;&gt; _is_matrix_spd(spd_matrix)\r\n    True\r\n    &quot;&quot;&quot;</span>\r\n    random_matrix = np.random.randn(dimension, dimension)\r\n    spd_matrix = np.dot(random_matrix, random_matrix.T)\r\n    <span class=\"hljs-keyword\">assert</span> _is_matrix_spd(spd_matrix)\r\n    <span class=\"hljs-keyword\">return</span> spd_matrix\r\n\r\n\r\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">conjugate_gradient</span>(<span class=\"hljs-params\">\r\n    spd_matrix: np.ndarray,\r\n    load_vector: np.ndarray,\r\n    max_iterations: <span class=\"hljs-built_in\">int</span> = <span class=\"hljs-number\">1000</span>,\r\n    tol: <span class=\"hljs-built_in\">float</span> = <span class=\"hljs-number\">1e-8</span>,\r\n</span>) -&gt; <span class=\"hljs-type\">Any</span>:\r\n    <span class=\"hljs-string\">&quot;&quot;&quot;\r\n    Returns solution to the linear system np.dot(spd_matrix, x) = b.\r\n\r\n    Input:\r\n    spd_matrix is an NxN Symmetric Positive Definite (SPD) matrix.\r\n    load_vector is an Nx1 vector.\r\n\r\n    Output:\r\n    x is an Nx1 vector that is the solution vector.\r\n\r\n    &gt;&gt;&gt; import numpy as np\r\n    &gt;&gt;&gt; spd_matrix = np.array([\r\n    ... [8.73256573, -5.02034289, -2.68709226],\r\n    ... [-5.02034289,  3.78188322,  0.91980451],\r\n    ... [-2.68709226,  0.91980451,  1.94746467]])\r\n    &gt;&gt;&gt; b = np.array([\r\n    ... [-5.80872761],\r\n    ... [ 3.23807431],\r\n    ... [ 1.95381422]])\r\n    &gt;&gt;&gt; conjugate_gradient(spd_matrix, b)\r\n    array([[-0.63114139],\r\n           [-0.01561498],\r\n           [ 0.13979294]])\r\n    &quot;&quot;&quot;</span>\r\n    <span class=\"hljs-comment\"># Ensure proper dimensionality.</span>\r\n    <span class=\"hljs-keyword\">assert</span> np.shape(spd_matrix)[<span class=\"hljs-number\">0</span>] == np.shape(spd_matrix)[<span class=\"hljs-number\">1</span>]\r\n    <span class=\"hljs-keyword\">assert</span> np.shape(load_vector)[<span class=\"hljs-number\">0</span>] == np.shape(spd_matrix)[<span class=\"hljs-number\">0</span>]\r\n    <span class=\"hljs-keyword\">assert</span> _is_matrix_spd(spd_matrix)\r\n\r\n    <span class=\"hljs-comment\"># Initialize solution guess, residual, search direction.</span>\r\n    x0 = np.zeros((np.shape(load_vector)[<span class=\"hljs-number\">0</span>], <span class=\"hljs-number\">1</span>))\r\n    r0 = np.copy(load_vector)\r\n    p0 = np.copy(r0)\r\n\r\n    <span class=\"hljs-comment\"># Set initial errors in solution guess and residual.</span>\r\n    error_residual = <span class=\"hljs-number\">1e9</span>\r\n    error_x_solution = <span class=\"hljs-number\">1e9</span>\r\n    error = <span class=\"hljs-number\">1e9</span>\r\n\r\n    <span class=\"hljs-comment\"># Set iteration counter to threshold number of iterations.</span>\r\n    iterations = <span class=\"hljs-number\">0</span>\r\n\r\n    <span class=\"hljs-keyword\">while</span> error &gt; tol:\r\n\r\n        <span class=\"hljs-comment\"># Save this value so we only calculate the matrix-vector product once.</span>\r\n        w = np.dot(spd_matrix, p0)\r\n\r\n        <span class=\"hljs-comment\"># The main algorithm.</span>\r\n\r\n        <span class=\"hljs-comment\"># Update search direction magnitude.</span>\r\n        alpha = np.dot(r0.T, r0) / np.dot(p0.T, w)\r\n        <span class=\"hljs-comment\"># Update solution guess.</span>\r\n        x = x0 + alpha * p0\r\n        <span class=\"hljs-comment\"># Calculate new residual.</span>\r\n        r = r0 - alpha * w\r\n        <span class=\"hljs-comment\"># Calculate new Krylov subspace scale.</span>\r\n        beta = np.dot(r.T, r) / np.dot(r0.T, r0)\r\n        <span class=\"hljs-comment\"># Calculate new A conjuage search direction.</span>\r\n        p = r + beta * p0\r\n\r\n        <span class=\"hljs-comment\"># Calculate errors.</span>\r\n        error_residual = np.linalg.norm(r - r0)\r\n        error_x_solution = np.linalg.norm(x - x0)\r\n        error = np.maximum(error_residual, error_x_solution)\r\n\r\n        <span class=\"hljs-comment\"># Update variables.</span>\r\n        x0 = np.copy(x)\r\n        r0 = np.copy(r)\r\n        p0 = np.copy(p)\r\n\r\n        <span class=\"hljs-comment\"># Update number of iterations.</span>\r\n        iterations += <span class=\"hljs-number\">1</span>\r\n        <span class=\"hljs-keyword\">if</span> iterations &gt; max_iterations:\r\n            <span class=\"hljs-keyword\">break</span>\r\n\r\n    <span class=\"hljs-keyword\">return</span> x\r\n\r\n\r\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_conjugate_gradient</span>() -&gt; <span class=\"hljs-literal\">None</span>:\r\n    <span class=\"hljs-string\">&quot;&quot;&quot;\r\n    &gt;&gt;&gt; test_conjugate_gradient()  # self running tests\r\n    &quot;&quot;&quot;</span>\r\n    <span class=\"hljs-comment\"># Create linear system with SPD matrix and known solution x_true.</span>\r\n    dimension = <span class=\"hljs-number\">3</span>\r\n    spd_matrix = _create_spd_matrix(dimension)\r\n    x_true = np.random.randn(dimension, <span class=\"hljs-number\">1</span>)\r\n    b = np.dot(spd_matrix, x_true)\r\n\r\n    <span class=\"hljs-comment\"># Numpy solution.</span>\r\n    x_numpy = np.linalg.solve(spd_matrix, b)\r\n\r\n    <span class=\"hljs-comment\"># Our implementation.</span>\r\n    x_conjugate_gradient = conjugate_gradient(spd_matrix, b)\r\n\r\n    <span class=\"hljs-comment\"># Ensure both solutions are close to x_true (and therefore one another).</span>\r\n    <span class=\"hljs-keyword\">assert</span> np.linalg.norm(x_numpy - x_true) &lt;= <span class=\"hljs-number\">1e-6</span>\r\n    <span class=\"hljs-keyword\">assert</span> np.linalg.norm(x_conjugate_gradient - x_true) &lt;= <span class=\"hljs-number\">1e-6</span>\r\n\r\n\r\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&quot;__main__&quot;</span>:\r\n    <span class=\"hljs-keyword\">import</span> doctest\r\n\r\n    doctest.testmod()\r\n    test_conjugate_gradient()\r\n"
    }
  },
  "contributors": [
    {
      "name": "Dhruv Manilawala",
      "email": "dhruvmanila@gmail.com",
      "commits": 1
    },
    {
      "name": "zakademic",
      "email": "67771932+zakademic@users.noreply.github.com",
      "commits": 1
    }
  ],
  "explanationUrl": {}
}